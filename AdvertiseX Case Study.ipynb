{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "994d9d40",
   "metadata": {},
   "source": [
    "#  <p style = \"text-align: center;\">AdvertiseX Case Study</p>\n",
    "### <p style = \"text-align: center;\"> By Hetansh Patel</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8461d2",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce93a17",
   "metadata": {},
   "source": [
    "## Solutions by Order:\n",
    "### Data Ingestion\n",
    "### Data Processing\n",
    "### Data Storage and Query Performance\n",
    "### Error Handling and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b202cf0",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa56de",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd92ab4",
   "metadata": {},
   "source": [
    "#### Goal: To build a scalable system capable of handling real-time and batch data (in JSON, CSV, and Avro formats) efficiently.\n",
    "#### Approaches(Possible):\n",
    "#### 1 )Utilize tools like Apache Kafka for real-time data streaming and Apache NiFi or Fluentd for flexible data routing and ingestion. Kafka can ingest data in various formats, enabling real-time processing and storage scalability.\n",
    "#### 2) Utilize cloud-native services like AWS Kinesis for streaming data and AWS Glue for data cataloging and ETL. These services offer scalability and ease of use, reducing the operational overhead of managing infrastructure.\n",
    "#### 3) Use serverless functions (e.g., AWS Lambda, Azure Functions) to ingest data. These can be triggered by event sources (such as new file uploads to cloud storage) and can process data formats on the fly before passing them to a streaming or storage service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfe7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching Data with Requests and Parsing with Pandas\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Fetching JSON data from an API\n",
    "response = requests.get('https://api.example.com/data')\n",
    "data = response.json()\n",
    "\n",
    "# Converting JSON data to a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Reading a CSV file directly into a Pandas DataFrame\n",
    "csv_df = pd.read_csv('path/to/your/file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing Messages to Kafka\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "p = Producer({'bootstrap.servers': 'your.kafka.broker:9092'})\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print('Message delivery failed:', err)\n",
    "    else:\n",
    "        print('Message delivered to', msg.topic(), msg.partition())\n",
    "\n",
    "# Sending a message to Kafka\n",
    "data = {'key': 'value'}\n",
    "p.produce('your_topic', key='your_key', value=str(data), callback=delivery_report)\n",
    "\n",
    "# Wait for any outstanding messages to be delivered\n",
    "p.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e8e3e9",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cea6cb",
   "metadata": {},
   "source": [
    "#### Goal: Standardize, enrich, and correlate data (impressions with clicks and conversions) for insights.\n",
    "#### Approaches(Possible):\n",
    "#### 1) A distributed data processing system that can handle batch and streaming data. It's ideal for complex transformations, aggregations, and data enrichment tasks, supporting both SQL-like queries and machine learning algorithms.\n",
    "#### 2) Google Cloud Dataflow for processing and transforming streaming and batch data efficiently, with managed services reducing operational load.\n",
    "#### Leverage Google BigQuery for its serverless, highly scalable, and cost-effective data warehouse capabilities, enabling advanced analytics directly on ingested data.\n",
    "#### 3) Apache Flink offers robust stream processing capabilities with a focus on event time processing and state management.\n",
    "#### Apache Beam provides an abstraction layer that allows for defining and executing data processing pipelines in a way that's independent of the backend technology, making it possible to switch between Flink, Spark, and others as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce29947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Data Processing with Dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Reading a large CSV file with Dask\n",
    "dask_df = dd.read_csv('path/to/large/file.csv')\n",
    "\n",
    "# Performing a transformation\n",
    "transformed_dask_df = dask_df.groupby('column').sum()\n",
    "\n",
    "# Compute results\n",
    "result = transformed_dask_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ca069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing with PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('app').getOrCreate()\n",
    "\n",
    "# Reading JSON data into a DataFrame\n",
    "df = spark.read.json('path/to/your/json')\n",
    "\n",
    "# Performing transformations\n",
    "transformed_df = df.groupBy('column').count()\n",
    "\n",
    "# Show results\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bd19a",
   "metadata": {},
   "source": [
    "# Data Storage and Query Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c576810",
   "metadata": {},
   "source": [
    "#### Goal: Efficiently store processed data for fast querying and analysis of ad campaign performance.\n",
    "#### Approaches(Possible):\n",
    "#### 1) Apache Cassandra for high availability and scalability, particularly for write-heavy workloads.\n",
    "#### Use Apache Parquet on a distributed file system (like HDFS or cloud storage) for efficient columnar storage, reducing storage costs and improving analytical query performance.\n",
    "#### 2) Services like Amazon Redshift, Google BigQuery, or Snowflake offer scalable and fully managed data warehouse solutions, with built-in capabilities for data management and optimization for analytics.\n",
    "#### 3) Implement a data lake using Amazon S3, Azure Data Lake Storage, or Google Cloud Storage to store raw data in various formats.\n",
    "#### Integrate with data lake analytics tools (like AWS Athena, Azure Synapse Analytics) to directly query and analyze data in the data lake without the need to transform or load it into a separate data warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SQLAlchemy for Database Operations\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine('sqlite:///example.db')\n",
    "\n",
    "# Write data to database (assuming df is a Pandas DataFrame)\n",
    "df.to_sql('table_name', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Read data from database\n",
    "loaded_df = pd.read_sql('SELECT * FROM table_name', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d899854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing and Reading ParquetFiles with Pandas and PyArrow\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df.to_parquet('path/to/store/data.parquet', engine='pyarrow')\n",
    "\n",
    "# Reading the parquet file\n",
    "loaded_df = pd.read_parquet('path/to/store/data.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b69958",
   "metadata": {},
   "source": [
    "# Error Handling and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f0cef",
   "metadata": {},
   "source": [
    "#### Goal: Detect and address data anomalies, discrepancies, or delays in real-time.\n",
    "#### Approaches(Possible):\n",
    "#### 1 ) ELK Stack (Elasticsearch, Logstash, Kibana) for logging, searching, and visualizing logs and data flows in real-time.\n",
    "#### Prometheus for monitoring metrics and setting up alerts based on thresholds or anomalies detected in the data pipeline performance.\n",
    "#### 2) Use cloud provider solutions like AWS CloudWatch, AWS X-Ray, Google Operations Suite (formerly Stackdriver), or Azure Monitor for comprehensive monitoring, logging, and diagnostics across all cloud resources and services.\n",
    "#### 3) Apache Airflow for workflow management, where you can define, schedule, and monitor data pipelines using directed acyclic graphs (DAGs). It includes the ability to alert on task failures or retries.\n",
    "#### Grafana for advanced analytics and monitoring with support for Prometheus as a data source, offering customizable dashboards for visualizing and analyzing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Python's Logging Module\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='app.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "try:\n",
    "    # Your code here\n",
    "    logging.info(\"Attempting to perform an operation\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with Sentry for Real-time Monitoring\n",
    "import sentry_sdk\n",
    "\n",
    "sentry_sdk.init(\n",
    "    dsn=\"your_sentry_dsn_here\",\n",
    "    traces_sample_rate=1.0\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Your code here that might fail\n",
    "    1 / 0\n",
    "except Exception as e:\n",
    "    sentry_sdk.capture_exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd135f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
